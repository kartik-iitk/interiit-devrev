{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U38_GxU5ZWFh"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyKtVXm661x3"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdk3n4f899fc"
      },
      "source": [
        "Load validation data for testing, based on missing data in the training data from squad 2.0 dataset. Round 1 data contains themes that are not present in training data. While, round 2 data contains themes that are present in training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5cqzE_kBBj_"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDsZ3veewUjM"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "def download_test_data(round = 1):\n",
        "    \"\"\"Download the test data (4 csv files)\"\"\"\n",
        "    assert round in [1,2], \"round can be 1 or 2\"\n",
        "    ids = [\n",
        "        [\n",
        "            \"15WPYOD3ZLShFq_NRtiBHbpz3RTvc8ZWR\",\n",
        "            \"15yxIF27NvEa3l12yNy6F5h8lGCJ2n7rf\",\n",
        "            \"1Ilpxyj_0T-1KzQMdVSEbSmc1ybxOv69G\",\n",
        "            \"1nkEDQZJY6_cAEVw3JlaKCgz0C6mDSYiv\"\n",
        "        ],\n",
        "        [\n",
        "            \"1-3fMldkBVsTAX3W5JewdAdlUG_agexG0\",\n",
        "            \"1-59pQe8TH7UaORF1RSqzFWybMJShdf1U\",\n",
        "            \"1-AbnJRRHQiTU5zyUdDC2gUwbIGkEF5l6\",\n",
        "            \"1-Px6FFj043L7lbAEBOAMSy2bdoPiVNhy\"\n",
        "        ]\n",
        "    ]\n",
        "    for id in ids[round-1]:\n",
        "        url = f\"https://drive.google.com/u/1/uc?id={id}&export=download\"\n",
        "        gdown.download(url, quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzHw72nd6_wA"
      },
      "source": [
        "### Generate Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FarPEbyBsIh"
      },
      "source": [
        "For a given theme, break its paragraphs into sentences and store their paragraph id. Load sentence encoder and calculate embeddings for the sentences from paragraphs and the queries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "g5jb4sYXKUvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-iuMv8R7BPv"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def para_to_sentences(para):\n",
        "    \"\"\"Splits a paragraph into sentences.\"\"\"\n",
        "    para = para.replace('\\n', ' ').replace('\\t', ' ').replace('\\x00', ' ')\n",
        "    return nltk.sent_tokenize(para)\n",
        "\n",
        "def load_sents_from_para(paras):\n",
        "    \"\"\"Spilits a list of paragraphs into sentences and returns the sentences\n",
        "    and their corresponding paragraph id\"\"\"\n",
        "    sents = []\n",
        "    para_id = []\n",
        "    for i,p in enumerate(paras):\n",
        "        new_sents = para_to_sentences(p['paragraph'])\n",
        "        sents += new_sents\n",
        "        para_id += [i]*len(new_sents)\n",
        "    return sents, para_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQKHrVVbDvwN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def load_encoder(encoder=\"universal-sentence-encoder-qa-v3\"):\n",
        "    \"\"\"Load Google's Universal Sentence Encoder for QA\"\"\"\n",
        "    if encoder == \"universal-sentence-encoder-qa-v3\":\n",
        "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder-qa/3\"\n",
        "        model = hub.load(module_url)\n",
        "    elif encoder == \"mpnet-base-v2\":\n",
        "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "    elif encoder == \"distilroberta-v1\":\n",
        "        model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
        "    elif encoder == \"minilm-l12-v2\":\n",
        "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
        "    else:\n",
        "        raise \"Unknown sentence encoder\"\n",
        "    return model\n",
        "\n",
        "def get_embeddings_guse(sents, paras, para_id, model, sents_type=\"Context\"):\n",
        "    \"\"\"Calculate embeddings for given list of sentences based on its type\n",
        "    i.e. either its a Question or a Context\"\"\"\n",
        "    if sents_type == \"Question\":\n",
        "        return model.signatures['question_encoder'](\n",
        "            tf.constant(sents)\n",
        "        )['outputs']\n",
        "    else:\n",
        "        contexts = [\n",
        "            paras[para_id[i]]['paragraph'] for i in range(len(sents))\n",
        "        ]\n",
        "        return model.signatures['response_encoder'](\n",
        "            input = tf.constant(sents),\n",
        "            context = tf.constant(contexts)             # can play with this\n",
        "        )['outputs']\n",
        "\n",
        "def get_embeddings_st(sents, model):\n",
        "    return model.encode(sents)\n",
        "\n",
        "def get_embeddings(encoder_name, sents, paras, para_id, model, sents_type=\"Context\"):\n",
        "    if encoder_name == \"universal-sentence-encoder-qa-v3\":\n",
        "        return get_embeddings_guse(sents, paras, para_id, model, sents_type)\n",
        "    elif encoder_name in [\"mpnet-base-v2\", \"distilroberta-v1\", \"minilm-l12-v2\"]:\n",
        "        return get_embeddings_st(sents, model)\n",
        "    else:\n",
        "        raise \"Unknown Sentence Encoder\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTEq0Khe7DM0"
      },
      "source": [
        "### Nearest Neighbour Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo6xW9HsHoMl"
      },
      "source": [
        "Based on the embeddings calculated, indexes them based on L2 distance and then applies nearest neighbour search to get top k closest sentences for each query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWSJfQaeHZlT"
      },
      "outputs": [],
      "source": [
        "!pip install -U  faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "def load_cross_encoder():\n",
        "    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', max_length=512)\n",
        "    return model"
      ],
      "metadata": {
        "id": "YDSxeL5t6C2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SwA5Hhc7JMO"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def get_k_nearest_neighbours(source_embeds, target_embeds, k = 10):\n",
        "    \"\"\"Returns k nearest neighbours of target_embeds in source_embeds\"\"\"\n",
        "    index = faiss.IndexFlatL2(source_embeds.shape[1])\n",
        "    index.add(np.array(source_embeds))\n",
        "    return index.search(np.array(target_embeds), k)\n",
        "\n",
        "def rerank(model, queries, sents, nearest_neighbours):    \n",
        "    new_nns = []\n",
        "    for q_idx, nns in enumerate(nearest_neighbours):\n",
        "        query = queries[q_idx]\n",
        "        data = []\n",
        "        for s_idx in nns:\n",
        "            data.append((query, sents[s_idx]))\n",
        "        scores = model.predict(data)\n",
        "        scores = sorted([(score, nns[i]) for i, score in enumerate(scores)])\n",
        "        new_ranks = [score[1] for score in scores]\n",
        "        new_nns.append(new_ranks)\n",
        "    return new_nns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check previously answered queries"
      ],
      "metadata": {
        "id": "Uxnd1xuWqVeA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSpVs8P8qoMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xccnbbAf7J1o"
      },
      "source": [
        "### Context Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJgYZNGdR5ni"
      },
      "source": [
        "Generates a context for a given query and its nearest neighbours. Also provides a method to get the paragraph id given the start idx of the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdyftt2i7SMV"
      },
      "outputs": [],
      "source": [
        "def get_context(query_id, query, sents, paras, para_ids, nearest_neighbours, option=1, m=1):\n",
        "    \"\"\"Generate the context for a given query and store the para_id for\n",
        "    each sentence\"\"\"\n",
        "    if option in [1, 2]:\n",
        "        if option == 1:\n",
        "            m = 0\n",
        "        context = \"\"\n",
        "        context_para_ids, sent_length = [], []\n",
        "        for sent_id in nearest_neighbours:\n",
        "            for j in range(-m, m+1):\n",
        "                cur_id = sent_id + j\n",
        "                if cur_id >= 0 and cur_id < len(para_ids) and para_ids[sent_id] == para_ids[cur_id]:\n",
        "                    context += sents[cur_id]\n",
        "                    context_para_ids.append(paras[para_ids[cur_id]]['id'])\n",
        "                    sent_length.append(len(sents[cur_id]))\n",
        "    # else:\n",
        "\n",
        "    sum = -1\n",
        "    for i in range(len(sent_length)):\n",
        "        sum += sent_length[i] + 1\n",
        "        sent_length[i] = sum\n",
        "    return context, context_para_ids, sent_length\n",
        "\n",
        "def para_id_retriever(start_idx, sent_length, context_para_ids):\n",
        "    \"\"\"Given start index of the answer, return the id of the paragraph\n",
        "    in which the answer belongs\"\"\"\n",
        "    if start_idx == -1:\n",
        "        return -1\n",
        "    for j in range(len(sent_length)):\n",
        "        if start_idx <= sent_length[j]:\n",
        "            return context_para_ids[j]\n",
        "    return context_para_ids[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raCs2XgbZpTG"
      },
      "source": [
        "### Load QA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actaNQ_CafqR"
      },
      "source": [
        "Given a theme, download the corresponding fine-tuned QA model and load the QA pipeline "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adapter-transformers"
      ],
      "metadata": {
        "id": "i2J3YAZsjn5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You will to restart the runtime here, due to import conflicts\n",
        "\n",
        "from transformers import AutoModelWithHeads\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from transformers.adapters import AutoAdapterModel\n",
        "from transformers.adapters import AdapterConfig\n",
        "\n",
        "def load_adapter_model_pipeline(adapter):\n",
        "  \n",
        "  if adapter == \"roberta-base-pf\":\n",
        "    model = AutoModelWithHeads.from_pretrained('roberta-base')\n",
        "    adapter_name = model.load_adapter('AdapterHub/roberta-base-pf-squad_v2', source='hf')\n",
        "    model.active_adapters = adapter_name\n",
        "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "  elif adapter == 'roberta-base-pf-ukp':\n",
        "    model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "    config = AdapterConfig.load(\"pfeiffer\")\n",
        "    adapter_name = model.load_adapter(\"qa/squad2@ukp\", config=config)\n",
        "    model.active_adapters = adapter_name\n",
        "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "  elif adapter == 'bart-large':\n",
        "    model = AutoAdapterModel.from_pretrained(\"facebook/bart-large\")\n",
        "    config = AdapterConfig.load(\"lohfink-rossi-leaveout\", non_linearity=\"relu\", reduction_factor=16)\n",
        "    adapter_name = model.load_adapter(\"qa/squad2@lohfink-rossi\", config=config)\n",
        "    model.active_adapters = adapter_name\n",
        "    tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
        "  elif adapter == 'bert-pf-ukp':\n",
        "    model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
        "    config = AdapterConfig.load(\"pfeiffer\")\n",
        "    adapter_name = model.load_adapter(\"qa/squad2@ukp\", config=config)\n",
        "    model.active_adapters = adapter_name\n",
        "    tokenizer == AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "  elif adapter == 'roberta-base-hl':\n",
        "    model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "    config = AdapterConfig.load(\"houlsby\")\n",
        "    adapter_name = model.load_adapter(\"qa/squad2@ukp\", config=config)\n",
        "    model.active_adapters = adapter_name\n",
        "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "  elif adapter == 'bert-hl-ukp':\n",
        "    model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
        "    config = AdapterConfig.load(\"houlsby\")\n",
        "    adapter_name = model.load_adapter(\"qa/squad2@ukp\", config=config)\n",
        "    model.active_adapters = adapter_name\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  qa = pipeline(\n",
        "      task='question-answering',\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      handle_impossible_answer=True,\n",
        "      # device=0\n",
        "  )\n",
        "\n",
        "  return qa\n"
      ],
      "metadata": {
        "id": "LIyT0iTpjozj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zfjkSO7apHi"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import json\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def load_model_links(json_link):\n",
        "    \"\"\"Downloads the JSON that contains the links to models and tokenizer\"\"\"\n",
        "    gdown.download(json_link, quiet=True)\n",
        "    with open('theme_wise_models.json') as f:\n",
        "        model_links = json.load(f)\n",
        "    return model_links\n",
        "\n",
        "def load_theme_model_pipeline(theme, tokenizer_link, model_links, model_id=\"\"):\n",
        "    \"\"\"Given a theme, loads the corresponding QA model\"\"\"\n",
        "    task = \"question-answering\"\n",
        "    if not model_id:\n",
        "        gdown.download(tokenizer_link, \"tokenizer.zip\", quiet=True)\n",
        "        gdown.download(model_links[theme]['link'], \"model.zip\", quiet=True)\n",
        "        with ZipFile(\"tokenizer.zip\", 'r') as zObject:\n",
        "            zObject.extractall()\n",
        "        with ZipFile(\"model.zip\", 'r') as zObject:\n",
        "            zObject.extractall()\n",
        "        tokenizer_path = \"tokenizer\"\n",
        "        model_path = model_links[theme]['path']\n",
        "        model = ORTModelForQuestionAnswering.from_pretrained(model_path)  # Vanilla model\n",
        "        model = ORTModelForQuestionAnswering.from_pretrained(\n",
        "            model_path, file_name=\"model_optimized_quantized.onnx\", provider=\"CUDAExecutionProvider\"\n",
        "        )\n",
        "        # model = ORTModelForQuestionAnswering.from_pretrained(\n",
        "        #     model_path, file_name=\"model_optimized_quantized.onnx\"\n",
        "        # )                                        # Optimized and Quantized Model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    else:\n",
        "        model = ORTModelForQuestionAnswering.from_pretrained(\n",
        "            model_id, from_transformers=True, provider=\"CUDAExecutionProvider\"\n",
        "        )\n",
        "        # model = ORTModelForQuestionAnswering.from_pretrained(\n",
        "        #     model_id, from_transformers=True\n",
        "        # )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    optimum_qa = pipeline(\n",
        "        task, model=model, tokenizer=tokenizer, handle_impossible_answer=True\n",
        "    )\n",
        "    return optimum_qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7Jcd0sw7TOf"
      },
      "source": [
        "### Run QA pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWsSMRk3kv5Z"
      },
      "source": [
        "Predicts the answer given query and context in the required format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cf_Mlpe7Z3L"
      },
      "outputs": [],
      "source": [
        "def predict(query_id, query, context, qa_model, pred_paras, sent_length, context_para_ids):\n",
        "    \"\"\"Predict the answer given a query and a context\"\"\"\n",
        "    prediction = qa_model(question=query, context=context)\n",
        "    ans = {\n",
        "        \"question_id\": query_id,\n",
        "        \"answers\": [prediction['answer']],\n",
        "        \"paragraph_id\": -1,\n",
        "        \"context\": context                # Extra info\n",
        "    }\n",
        "    if prediction['answer'] != \"\":\n",
        "        ans[\"paragraph_id\"] = para_id_retriever(\n",
        "            prediction['start'], sent_length, context_para_ids\n",
        "        )\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_context(passes, context, sent_length):\n",
        "    context_list = []\n",
        "    rem_sents = len(sent_length)\n",
        "    passes = min(passes, rem_sents)\n",
        "    passes_left = passes\n",
        "    i, end = -1, -1\n",
        "    for j in range(passes):\n",
        "        i += int(rem_sents / passes_left)\n",
        "        context_list.append(context[end+1:sent_length[i]])\n",
        "        end = sent_length[i]\n",
        "        rem_sents -= rem_sents / passes_left\n",
        "        passes_left -= 1\n",
        "    return context_list\n",
        "\n",
        "def get_best_prediction(query, context_list, qa_model):\n",
        "    best_prediction = {'answer': \"\"}\n",
        "    text_start = 0\n",
        "    for context in context_list:\n",
        "        prediction = qa_model(question=query, context=context)\n",
        "        if prediction['answer'] != \"\" and (best_prediction['answer'] == \"\" or prediction['score'] > best_prediction['score']):\n",
        "            prediction['start'] += text_start\n",
        "            best_prediction = prediction\n",
        "        text_start += len(context)\n",
        "    return best_prediction\n",
        "\n",
        "def multiple_pass_prediction(passes, query_id, query, context, qa_model, pred_paras, sent_length, context_para_ids):\n",
        "    context_list = divide_context(passes, context, sent_length)\n",
        "    prediction = get_best_prediction(query, context_list, qa_model)\n",
        "    ans = {\n",
        "        \"question_id\": query_id,\n",
        "        \"answers\": [prediction['answer']],\n",
        "        \"paragraph_id\": -1,\n",
        "        \"context\": context                # Extra info\n",
        "    }\n",
        "    if prediction['answer'] != \"\":\n",
        "        ans[\"paragraph_id\"] = para_id_retriever(\n",
        "            prediction['start'], sent_length, context_para_ids\n",
        "        )\n",
        "    return ans"
      ],
      "metadata": {
        "id": "2y-4btD4GC0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlfTtpesQFcK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def predict_theme_wise(paras, ques, pred_out, encoder_name, sents_encoder, qa_pipeline, ctx_option, k, m, qa_passes):\n",
        "    \"\"\"Predicts the answers for all queries of a particular theme\"\"\"\n",
        "    ann_inference_time, qna_inference_time = 0., 0.\n",
        "    theme = ques[0][\"theme\"]\n",
        "    print(f'Theme: {theme}')\n",
        "\n",
        "    # Preprocessing of contexts\n",
        "    sents, para_id = load_sents_from_para(paras)\n",
        "    sents_embed = get_embeddings(\n",
        "        encoder_name, sents, paras, para_id, sents_encoder, sents_type=\"Context\"\n",
        "    )\n",
        "\n",
        "    # Nearest Neighbour Search\n",
        "    start_time = time.time()\n",
        "    ques_list = [q['question'] for q in ques]\n",
        "    ques_embed = get_embeddings(\n",
        "        encoder_name, ques_list, None, None, sents_encoder, sents_type=\"Question\"\n",
        "    )\n",
        "    D, I = get_k_nearest_neighbours(sents_embed, ques_embed, k)\n",
        "    ann_inference_time = (time.time() - start_time)*1000.\n",
        "\n",
        "    pred_paras = [\n",
        "        [paras[para_id[sent_idx]]['id'] for sent_idx in I[i]]\n",
        "        for i in range(len(I))\n",
        "    ]\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i in tqdm(range(len(ques))):\n",
        "        q = ques[i]\n",
        "\n",
        "        # Context Generation\n",
        "        context, context_para_ids, sent_length = get_context(\n",
        "            q[\"id\"], q['question'], sents, paras, para_id, I[i], ctx_option, m\n",
        "        )\n",
        "\n",
        "        # Answer Prediction and Paragraph Retrieval\n",
        "        ans = multiple_pass_prediction(\n",
        "            qa_passes, q[\"id\"], q['question'], context, qa_pipeline,\n",
        "            pred_paras[i], sent_length, context_para_ids\n",
        "        )\n",
        "        pred_out.append(ans)\n",
        "\n",
        "    # Print Inference Time\n",
        "    qna_inference_time = (time.time() - start_time)*1000.\n",
        "    print(\n",
        "        f'Avg. ANN IT = {round(ann_inference_time/len(ques), 2)} ms, ' +\n",
        "        f'Avg. QnA IT = {round(qna_inference_time/len(ques),2)} ms\\n'\n",
        "    )\n",
        "    return (ann_inference_time, qna_inference_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHsjvLAgTn3g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def predict_multiple_themes(params):\n",
        "    \"\"\"Predicts the answers for queries from multiple (num_themes) themes\"\"\"\n",
        "    # Load paras and queries\n",
        "    paragraphs = json.loads(pd.read_csv(\"input_paragraph.csv\").to_json(orient=\"records\"))\n",
        "    questions = json.loads(pd.read_csv(\"input_question.csv\").to_json(orient=\"records\"))\n",
        "    theme_intervals = json.loads(pd.read_csv(\"theme_interval.csv\").to_json(orient=\"records\"))\n",
        "    pred_out = []\n",
        "    theme_inf_time = {}\n",
        "    \n",
        "    # Number of themes for prediction\n",
        "    if params['num_themes'] == -1 or params['num_themes'] > len(theme_intervals):\n",
        "        params['num_themes'] = len(theme_intervals)\n",
        "    \n",
        "    # if using pretrained model\n",
        "    if params['use_pretrained']:\n",
        "        # qa_pipeline = load_optimized_model_pipeline(\n",
        "        #     params['model_id'], '/content/model.onnx', params['use_onnx'], params['optimize'], params['quantize']\n",
        "        # )\n",
        "\n",
        "        qa_pipeline = load_adapter_model_pipeline(params['model_id'])\n",
        "\n",
        "    # Predict for each theme\n",
        "    for theme_interval in theme_intervals[:params['num_themes']]:\n",
        "        theme = theme_interval[\"theme\"]\n",
        "        if not params['use_pretrained']:\n",
        "            qa_pipeline = load_theme_model_pipeline(\n",
        "                theme, params['tok_link'], params['qam_links'], \n",
        "            )\n",
        "        theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n",
        "        theme_paras = [p for p in paragraphs if p[\"theme\"] == theme]\n",
        "        execution_time = predict_theme_wise(\n",
        "            theme_paras, theme_ques, pred_out, params['encoder_name'], params['encoder'],\n",
        "            qa_pipeline, params['ctx_option'], params['k'], params['m'], params['qa_passes']\n",
        "        )\n",
        "        theme_inf_time[theme] = execution_time\n",
        "    \n",
        "    # Export predictions\n",
        "    pred_df = pd.DataFrame.from_records(pred_out)\n",
        "    pred_df.to_csv('output_prediction.csv', index=False)\n",
        "\n",
        "    return theme_inf_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNJggLTqUSrg"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCkWmI4ZUU8i"
      },
      "source": [
        "Evaluates and prints statistics of the predictions by the given pipeline. Metrics include the F1 Score, Paragraph Accuracy, Mean Rank of the gold paragraph, performance on true positives and negatives, inference times, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI9fmizthXlR"
      },
      "outputs": [],
      "source": [
        "import string, re\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\" \n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "    \n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    \n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    \n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    \n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "\n",
        "def calc_f1(a_gold, a_pred):\n",
        "    \"\"\"Calulates F1 score, given prediction and a gold answer\"\"\"\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    \n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    \n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    \n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def calc_max_f1(predicted, ground_truths):\n",
        "    \"\"\"Calulates the max F1 score, given prediction and the gold answers\"\"\"\n",
        "    max_f1 = 0\n",
        "    for ground_truth in ground_truths:\n",
        "        f1 = calc_f1(str(predicted), str(ground_truth))\n",
        "        max_f1 = max(max_f1, f1)\n",
        "    return max_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MafV6F1JnSgo"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_metrics():\n",
        "    \"\"\"Calculate metrics using the predictions and the ground truths\"\"\"\n",
        "    metrics = {}\n",
        "    # Load questions, prediction and ground_truth csv\n",
        "    questions = pd.read_csv(\"input_question.csv\")\n",
        "    pred = pd.read_csv(\"output_prediction.csv\")\n",
        "    truth = pd.read_csv(\"ground_truth.csv\")\n",
        "    \n",
        "    # String to list and numbers conversion\n",
        "    truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n",
        "    truth.answers = truth.answers.apply(literal_eval)\n",
        "    pred.answers = pred.answers.apply(literal_eval)\n",
        "\n",
        "    # Go thorugh each prediction and update the metrics\n",
        "    for idx in pred.index:\n",
        "        q_id = pred[\"question_id\"][idx]\n",
        "        q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n",
        "        theme = q_rows[\"theme\"]\n",
        "        predicted_paragraph = pred[\"paragraph_id\"][idx]\n",
        "        predicted_ans = pred[\"answers\"][idx][0]\n",
        "        \n",
        "        if theme not in metrics.keys():\n",
        "            metrics[theme] = {\n",
        "                \"total_positive\": 0,\n",
        "                \"total_negative\": 0,\n",
        "                \"true_positive\": 0,\n",
        "                \"true_negative\": 0,\n",
        "                'ansInCtx': 0,\n",
        "                \"total_predictions\": 0,\n",
        "                \"f1_sum\": 0\n",
        "            }\n",
        "\n",
        "        truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n",
        "        truth_paragraph_id = [ int(i) for i in truth_row[\"paragraph_id\"] ]\n",
        "\n",
        "        if truth_paragraph_id == []:\n",
        "            metrics[theme][\"total_negative\"] += 1\n",
        "        else:\n",
        "            metrics[theme][\"total_positive\"] += 1\n",
        "            for ans in truth_row[\"answers\"]:\n",
        "                if ans in pred['context'][idx]:\n",
        "                    metrics[theme][\"ansInCtx\"] += 1\n",
        "                    break\n",
        "\n",
        "        if predicted_paragraph in truth_paragraph_id:\n",
        "            # Increase TP for that theme.\n",
        "            metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n",
        "        \n",
        "        # -1 prediction in case there is no paragraph which can answer the query.\n",
        "        if predicted_paragraph == -1 and truth_paragraph_id == []:\n",
        "            # Increase TN.\n",
        "            metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n",
        "\n",
        "        # Increase total predictions for that theme.\n",
        "        metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n",
        "        if truth_row[\"answers\"] == []:\n",
        "            truth_row[\"answers\"] = [\"\"]\n",
        "        f1 = calc_max_f1(predicted_ans, truth_row[\"answers\"])\n",
        "        metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-WRArKplZ4o"
      },
      "outputs": [],
      "source": [
        "def show(val, sz, dec = 0):\n",
        "    \"\"\"Prints the value and adds whitespaces so that characters printed = sz\"\"\"\n",
        "    val_str = str(round(val, dec))\n",
        "    return ' '*(max(0, sz - len(val_str))) + val_str\n",
        "\n",
        "\n",
        "def calculate_score(theme_inf_time, inf_time_threshold = 1000.0):\n",
        "    \"\"\"Calculates and prints theme-wise as well as aggregated metrics score\"\"\"\n",
        "    metrics = evaluate_metrics()\n",
        "    final_para_score = 0.0\n",
        "    final_qa_score = 0.0\n",
        "    q, aic, tait, tqit, totp, totn, tp, tn, tf1 = 0, 0, 0., 0., 0, 0, 0, 0, 0.\n",
        "\n",
        "    print('Theme             | Queries |  AIT: (ANN) + (QnA) = Total | ansInCtx'\n",
        "        ' % | TP % (TotP) | TN % (TotN) | Para Acc | Final PA | F1 Score | '\n",
        "        'Final F1'\n",
        "    )\n",
        "    print('------------------|---------|-----------------------------|-------'\n",
        "        '-----|-------------|-------------|----------|----------|----------|'\n",
        "        '---------'\n",
        "    )\n",
        "\n",
        "    # Print theme wise metrics score\n",
        "    for theme in metrics:\n",
        "        inf_time_score = 1.0\n",
        "        metric = metrics[theme]\n",
        "        para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"] \n",
        "        qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n",
        "        avg_ann_inf_time = theme_inf_time[theme][0] / metric[\"total_predictions\"]\n",
        "        avg_qna_inf_time = theme_inf_time[theme][1] / metric[\"total_predictions\"]\n",
        "\n",
        "        avg_inf_time = avg_ann_inf_time + avg_qna_inf_time\n",
        "        if avg_inf_time > inf_time_threshold:\n",
        "            inf_time_score = inf_time_threshold / avg_inf_time\n",
        "\n",
        "        q += metric[\"total_predictions\"]\n",
        "        aic += metric[\"ansInCtx\"]\n",
        "        tait += theme_inf_time[theme][0]\n",
        "        tqit += theme_inf_time[theme][1]\n",
        "        totp += metric[\"total_positive\"]\n",
        "        totn += metric[\"total_negative\"]\n",
        "        tp += metric[\"true_positive\"]\n",
        "        tn += metric[\"true_negative\"]\n",
        "        tf1 += metric[\"f1_sum\"]\n",
        "        final_qa_score += inf_time_score * qa_score\n",
        "        final_para_score += inf_time_score * para_score\n",
        "\n",
        "        print(f'{(theme + \" \"*17)[:17]} | '\n",
        "            f'{show(metric[\"total_predictions\"],7)} | '\n",
        "            f'{show(avg_ann_inf_time,6,2)} + {show(avg_qna_inf_time,6,2)} = '\n",
        "            f'{show(avg_inf_time,6,2)} ms | '\n",
        "            f'{show(metric[\"ansInCtx\"]*100./metric[\"total_positive\"],8,2)} '\n",
        "            f'% | {show(int(metric[\"true_positive\"]*100./max(1,metric[\"total_positive\"])),3)}% '\n",
        "            f'({show(metric[\"total_positive\"],4)}) | '\n",
        "            f'{show(int(metric[\"true_negative\"]*100./max(1,metric[\"total_negative\"])),3)}% '\n",
        "            f'({show(metric[\"total_negative\"],4)}) | '\n",
        "            f'{show(para_score,8,4)} | {show(inf_time_score*para_score,8,5)} | '\n",
        "            f'{show(qa_score,8,4)} | {show(inf_time_score*qa_score,8,5)}')\n",
        "\n",
        "    final_qa_score /= len(metrics)\n",
        "    final_para_score /= len(metrics)\n",
        "    # Print Aggregated Metrics Score\n",
        "    print(f'------------------|---------|-----------------------------|'\n",
        "        f'------------|-------------|-------------|----------|----------'\n",
        "        f'|----------|---------')\n",
        "    print(f'Grand Total       | {show(q,7)} | {show(tait/q,6,2)} + '\n",
        "        f'{show(tqit/q,6,2)} = {show((tait+tqit)/q,6,2)} ms |'\n",
        "        f'{show(aic*100./totp,9,2)} % | {show(int(tp*100./max(1,totp)),3)}% '\n",
        "        f'({show(totp,4)}) | {show(int(tn*100./max(1,totn)),3)}% '\n",
        "        f'({show(totn,4)}) | {show((tp+tn)/q,8,4)} | {show(final_para_score,8,5)} | '\n",
        "        f'{show(tf1/q,8,4)} | {show(final_qa_score,8,5)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q_1CZAKZZ1w"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data 1 contains queries for new themes, while data 2 contains queries for old themes\n",
        "validation_data = 1 #@param [\"1\", \"2\"] {type:\"raw\"}\n",
        "# Choose -1 to test on all themes\n",
        "num_themes_to_test = 10 #@param {type:\"integer\"}\n",
        "\n",
        "sentence_encoder = \"mpnet-base-v2\" #@param [\"universal-sentence-encoder-qa-v3\", \"mpnet-base-v2\", \"distilroberta-v1\", \"minilm-l12-v2\"]\n",
        "indexing_library = \"faiss\" #@param [\"faiss\"]\n",
        "search_previously_answered_queries = False #@param {type:\"boolean\"}\n",
        "context_generation = \"top-k nearest sentences\" #@param [\"top-k nearest sentences\", \"top-k nearest sentences with window of m sentences\", \"paragraphs of top k sentences\"]\n",
        "context_option = 1\n",
        "if context_generation == \"top-k nearest sentences with window of m sentences\":\n",
        "    context_option = 2\n",
        "elif context_generation == \"paragraphs of top k sentences\":\n",
        "    context_option = 3\n",
        "k = 7 #@param {type:\"slider\", min:1, max:15, step:1}\n",
        "m = 1 #@param {type:\"slider\", min:1, max:3, step:1}\n",
        "context_similarity_threshold = 2 #@param {type:\"number\"}\n",
        "\n",
        "use_pretrained_model_for_QA = True #@param {type:\"boolean\"}\n",
        "model_id = \"roberta-base-pf-ukp\" #@param [\"roberta-base-pf\", \"roberta-base-pf-ukp\", \"bart-large\", \"bert-pf-ukp\", \"roberta-base-hl\", \"bert-hl-ukp\"]\n",
        "use_onnx = True #@param {type:\"boolean\"}\n",
        "optimize_model = True #@param {type:\"boolean\"}\n",
        "quantize_model = False #@param {type:\"boolean\"}\n",
        "tokenizer_link = \"https://drive.google.com/u/1/uc?id=1Rq9kXnOpbY1FsDBjHtlx4i7scrnk_0A9&export=download\" #@param {type:\"string\"}\n",
        "model_links_json = \"https://drive.google.com/u/1/uc?id=1usU8GcPTzIakelkJd7ChvQGxqwlEJxoz&export=download\" #@param {type:\"string\"}\n",
        "num_qa_passes = 1 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "\n",
        "# download validation data\n",
        "download_test_data(round = validation_data)\n",
        "\n",
        "# load sentence encoder (Google's Universal Sentence Encoder for QA)\n",
        "sents_encoder = load_encoder(sentence_encoder)\n",
        "\n",
        "# load QA models link (if using fine-tuned models)\n",
        "qa_model_links = load_model_links(model_links_json)\n",
        "\n",
        "params = {\n",
        "    'encoder_name': sentence_encoder,\n",
        "    'encoder': sents_encoder,\n",
        "    'qam_links': qa_model_links,\n",
        "    'tok_link': tokenizer_link,\n",
        "    'ctx_option': context_option,\n",
        "    'k': k,\n",
        "    'm': m,\n",
        "    'use_pretrained': use_pretrained_model_for_QA,\n",
        "    'model_id': model_id,\n",
        "    'use_onnx': use_onnx,\n",
        "    'optimize': optimize_model,\n",
        "    'quantize': quantize_model,\n",
        "    'num_themes': num_themes_to_test,\n",
        "    'qa_passes': num_qa_passes\n",
        "}"
      ],
      "metadata": {
        "id": "i2sheww6oVzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL1j7Ll5ce7d"
      },
      "outputs": [],
      "source": [
        "theme_inf_time = predict_multiple_themes(params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_score(theme_inf_time, inf_time_threshold = 1000.0) # with mpnet and bert-base-uncased ukp / bert-base_qa_squad2_houlsby"
      ],
      "metadata": {
        "id": "ydviz9fldQR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JyKtVXm661x3",
        "UzHw72nd6_wA",
        "BTEq0Khe7DM0",
        "Uxnd1xuWqVeA",
        "xccnbbAf7J1o"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}