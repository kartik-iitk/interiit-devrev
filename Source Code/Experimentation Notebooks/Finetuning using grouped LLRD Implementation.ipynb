{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1PSyNND4a4Q"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcBT6tLgYwwm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install -U transformers\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh30HOkS4mC0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, BertTokenizerFast, BertForQuestionAnswering, AutoModelForQuestionAnswering\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_scheduler\n",
        "from transformers import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0r8paULkIC"
      },
      "source": [
        "####  Data Loading."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading cluster-wise csvs\n",
        "# %%capture\n",
        "! gdown https://drive.google.com/drive/folders/1gebbrob3ssLaVivSE9NQNVjfC4VQ_w5T?usp=share_link -O . --folder #first batch\n",
        "! gdown https://drive.google.com/drive/folders/1TTcl6weFkOy8EPb3DVKteZK0KF9guALj?usp=share_link -O . --folder #second batch"
      ],
      "metadata": {
        "id": "Cjwf_L3liuMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18efe39-b96f-40f8-b132-7e51c0adbbe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1LXFdB38nK0EJBCYuDkk9ilFQdyhgno1K 0_test.csv\n",
            "Processing file 1-iyVF0xsZxWEO3fqwjOdQKUw1qmZ6DJM 0_train.csv\n",
            "Processing file 1wqgOw5Seoio2AjwFfFPD9_Yn83-hDwbe 1_test.csv\n",
            "Processing file 17k0_r9Y7xbyVoHUYRnMWxgGAKeBlD0qe 1_train.csv\n",
            "Processing file 1sADLTPSdS5w67SIQ96zvIELTLYS6qWOo 2_test.csv\n",
            "Processing file 1gyFqNov0B9C-XXlYzX78gYCG74q8plun 2_train.csv\n",
            "Processing file 1nSUBKXYtKyGi6wSP0TxGd02UlHpT8fmQ 3_test.csv\n",
            "Processing file 1CdyNEbjtJ4gLuw92-bHRIHT9J6c4JfVm 3_train.csv\n",
            "Processing file 173JKDoq-dHN_C-Q092HhwgR-i3q_QZFy 4_test.csv\n",
            "Processing file 1bhJtlTOJHrv_GfNXus-iG9xdOFosxj3W 4_train.csv\n",
            "Processing file 1_IShR1DUXCx0Vpat2Br288gHBHVyepoF 5_test.csv\n",
            "Processing file 1xRJFpE_5zukcAHGO3pfza61FB8tPm1CE 5_train.csv\n",
            "Processing file 1-kEW8i_qtDS9PCHkjKmdHIH8UM1oJhLE 6_test.csv\n",
            "Processing file 11nivvefLM5DD92SM2zxlfL-IN8IAl2Zf 6_train.csv\n",
            "Processing file 1owBfZC3nrX4l5CZMKKCZ75DeRlywXcf9 7_test.csv\n",
            "Processing file 1WGo5ukU6cwISDFE910sL4zwbYZpGolwN 7_train.csv\n",
            "Processing file 1kIAn5t7RV8eGcUMIe6soLD5arWbsllvy 8_test.csv\n",
            "Processing file 1QmRUSZ3U4AgjQsaSyEd26LmVhhtZJ0kK 8_train.csv\n",
            "Processing file 11DPVX0LrC6qbw8WuTrXqg_EWRf4XToS0 9_test.csv\n",
            "Processing file 1bHrd7_cVP8KzOqeFFCFMQLqbuY-yhJ-z 9_train.csv\n",
            "Processing file 1MHW4dTaVB1ReHfsvFZuGDYsr0JRUbROI 11_test.csv\n",
            "Processing file 11_SXYyILf70609yWB2CQsgo6JUXut-cz 11_train.csv\n",
            "Processing file 1BnGnQWfgl1kwK3CqT9snA0-elysrTt3A 12_test.csv\n",
            "Processing file 1lbLG1pW-VH6DhHG37tpPeTLlsVWNFs5_ 12_train.csv\n",
            "Processing file 190cZqbVVXa--oCn87SYfkJsaD-SWMIDa 13_test.csv\n",
            "Processing file 1mro699Fx2iAS2plUqHz_pByRGmKMnX-_ 13_train.csv\n",
            "Processing file 1Zq2-rdpS_sNMD5EzUGLstR4CqP2_-XIW 14_test.csv\n",
            "Processing file 1QF7pbj9LzlMeVNCOstpG-4ZSs6DqQ0fy 14_train.csv\n",
            "Processing file 1w-oJn0_Fy76W1ULIW3HpnT2PmiiCGJke 15_test.csv\n",
            "Processing file 11GjDQMuNBkxE8YaZQ2js3i0VSM0cfoui 15_train.csv\n",
            "Processing file 12VcDQOF7ECSEQIr0tvOu9dKa8Kq-Fb9s 16_test.csv\n",
            "Processing file 1blZn59Z3BeL79NoOl4flWuRTsNrS2lxY 16_train.csv\n",
            "Processing file 1WJA6sTE6SyVO_8HRt_IgCMSy9kfSlpQe 17_test.csv\n",
            "Processing file 1xSibkKEXLB_Lq6CiUVru8aBA3gl52U-f 17_train.csv\n",
            "Processing file 1YCeDkUQI9d7BBe5PvVaXHlE-M6No3tLS 18_test.csv\n",
            "Processing file 1lSUROv2rWSgsi7wLIQYfdEwZPLKVBKBn 18_train.csv\n",
            "Processing file 1xETg4SwjHgqCQvEOCZqtIDbdJBrHOY_b 19_test.csv\n",
            "Processing file 16xjty4dlX_rMXTNGNM1r81_pISGck1rR 19_train.csv\n",
            "Processing file 1EdahSBtnBrCfwa2WG9e9lSlnyrV4nxoA 20_test.csv\n",
            "Processing file 1lE2o147l2XkdGSFS9_HPWtDmdKhnVqKc 20_train.csv\n",
            "Processing file 1xjc30_eXaJ1no0Gip4Duo8gpC9mqrpc- 21_test.csv\n",
            "Processing file 1g7xROXkf-J4AWI7sMxHxo1eR6HilyPol 21_train.csv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1LXFdB38nK0EJBCYuDkk9ilFQdyhgno1K \n",
            "\n",
            "Download ended unsuccessfully\n",
            "Retrieving folder list\n",
            "Retrieving folder 110MNanJ77AM6e0Yj9136_MdHRPDTdN8R .ipynb_checkpoints\n",
            "Processing file 1RAyFKJ9Gm944_W-sU5TDnRuIq5XrX3o6 22_test.csv\n",
            "Processing file 1U-Ign22NcNkPbP9fvHk3IxxhPVbWOztC 22_train.csv\n",
            "Processing file 1rVEQC1Ne3rjqGKnzv7YGrPiiXm7IqUMd 23_test.csv\n",
            "Processing file 1YgPT0bBs_B11yTcMKK1kF2vBNCwMf6_k 23_train.csv\n",
            "Processing file 1HnkK2L2HCr0X3bq0DENWsJzkg-MRLB7z 24_test.csv\n",
            "Processing file 1gPkIzsnTbV3tQlRgMMB8kqohMrSftV1R 24_train.csv\n",
            "Processing file 1YXD6pSPZfXZ7quVDyeEuMwah9TCCUKKe 25_test.csv\n",
            "Processing file 10VYlLjqbcqVwS0V5NEDUNj3I_JQ9opgj 25_train.csv\n",
            "Processing file 1aGDcVPYzYvoKbBn9uFK0PWgArTsnBJCw 26_test.csv\n",
            "Processing file 1OgnaWdnMpq3q7sFCr9SFvQkvNhsKHQPx 26_train.csv\n",
            "Processing file 1gL-IRwrmPpL00ZNnTNsXy7BBp-WTE1Gc 28_test.csv\n",
            "Processing file 1Bi6k4tH3auq85ZRtJWhxZB7eP_ijnlAw 28_train.csv\n",
            "Processing file 13knGW8Y1vsLioF7gsBRdzAz6iVJPWPfs 29_test.csv\n",
            "Processing file 11hOv6WQynaK1dfzo0YtmBe-9pjJZgBCj 29_train.csv\n",
            "Processing file 1SdOWyqg44u18Cf1X4wQmJyFO06t87E-w 31_test.csv\n",
            "Processing file 15eUxS24nperjNSl0VnpShibYXauKMVQb 31_train.csv\n",
            "Processing file 16pCEkV0NQm-y3fyMLrMTi51ws7zUHK0d 32_test.csv\n",
            "Processing file 1fRo1lSaaIkIlCZ9SQDarrsvJbWkJIN69 32_train.csv\n",
            "Processing file 1b7-FEl4gIN5Eh5_zH_rU5OTEHHpSZhgk 33_test.csv\n",
            "Processing file 1EWUOHLmkUKee2Z__MU_yk9By8Lv2IFjc 33_train.csv\n",
            "Processing file 1CMgCxxgSNmlfRDXbBkzCODya6GCotWWg 34_test.csv\n",
            "Processing file 1mGftpxP4urbUHdBqWP2RHFcz1LXkK6Vn 34_train.csv\n",
            "Processing file 1yqqJ7RtIe9F0zKYTpM8SQPKSyFMjeacm 35_test.csv\n",
            "Processing file 16-8FnXS0IURKV3hVAEyTeHnuOGmTaflH 35_train.csv\n",
            "Processing file 162rlFH9oQfCNcDnuvGEwfrEexhXAuYzx 36_test.csv\n",
            "Processing file 1azPyYw8E3XhwliEgYVBOhemfa9xW5ac3 36_train.csv\n",
            "Processing file 19DwQhpvpKdJ1AJB167L8NuCmL_x81GSQ 37_test.csv\n",
            "Processing file 1sM-_jn1mLuxtR1mKof12R85J2t6QS2wZ 37_train.csv\n",
            "Processing file 1PUq4_NHrEkMWEDQrBw1i9_-f--_LZ3MA 38_test.csv\n",
            "Processing file 1V6hpEpDAZiMAMzw6rm1K14dCixsxrnKX 38_train.csv\n",
            "Processing file 1mmWHTKl5HRv3tBFUGzE3B-VdlkVCYIcp 39_test.csv\n",
            "Processing file 1aVYRs9r5qHt7B1zB9k_CEPpehVB5Prb6 39_train.csv\n",
            "Processing file 155uabb4zf0lrvi5zUaGUyenvMcpdII3R 40_test.csv\n",
            "Processing file 1cAdRfIvxw578fB8Cngfv-dsErq4ZOcn4 40_train.csv\n",
            "Processing file 1YgnHbJ4NRFRxhmSR7Q0VK38yQq5vjePT 41_test.csv\n",
            "Processing file 1519VBAuahS0X_dT1I4XbH5qVfR6RaQT6 41_train.csv\n",
            "Processing file 1zUWHilicPipq-MkrnfFyn3Lil0AcBMxc 42_test.csv\n",
            "Processing file 1MqlrfAzHeQusj7FfBPEupcuOYYLwNvx3 42_train.csv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1RAyFKJ9Gm944_W-sU5TDnRuIq5XrX3o6 \n",
            "\n",
            "Download ended unsuccessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mr4nTU4dpyu"
      },
      "outputs": [],
      "source": [
        "def loaddataset(cluster = 0):\n",
        "  try:\n",
        "    base_url = '1/'\n",
        "    data_files = {\"train\":base_url + \"{}_train.csv\".format(cluster), \"test\": base_url + \"{}_test.csv\".format(cluster)}\n",
        "    # data_files = {\"train\":base_url + \"{}_train.csv\".format(theme), \"test\": base_url + \"{}_test.csv\".format(theme)}\n",
        "    dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "  except:\n",
        "    base_url = '2/'\n",
        "    data_files = {\"train\":base_url + \"{}_train.csv\".format(cluster), \"test\": base_url + \"{}_test.csv\".format(cluster)}\n",
        "    # data_files = {\"train\":base_url + \"{}_train.csv\".format(theme), \"test\": base_url + \"{}_test.csv\".format(theme)}\n",
        "    dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQaYyHGvxbW2",
        "outputId": "68b14625-afb9-4db2-b2ef-0c0c7f22e753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  2  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNrvZPOZ6Bqn"
      },
      "source": [
        "\n",
        "#### Finetuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQemcukMPZz"
      },
      "source": [
        "##### Device Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YOQzlfiMjEO"
      },
      "outputs": [],
      "source": [
        "#setting up global variables\n",
        "model_name = \"PremalMatalia/electra-base-best-squad2\"\n",
        "batch_size = 8\n",
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "data_collator = default_data_collator\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_device(model_name):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast), \"Fast Tokenizer Needed\"\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "  pad_on_right = tokenizer.padding_side == \"right\"\n",
        "  model.to(device)\n",
        "  return model, tokenizer, pad_on_right\n"
      ],
      "metadata": {
        "id": "rxsKYTYBmAsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ZX77CuxDxr"
      },
      "source": [
        "\n",
        "##### Preprocessing for FineTuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwEoL09kxBcC"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_training(dataset, tokenizer, pad_on_right):\n",
        "    dataset[\"Question\"] = [q.lstrip() for q in dataset[\"Question\"]]\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        dataset[\"Question\" if pad_on_right else \"Paragraph\"],\n",
        "        dataset[\"Paragraph\" if pad_on_right else \"Qustion\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "    # print(sample_mapping, offset_mapping)\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        #! WORKS\n",
        "\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = {\n",
        "            'text' : (dataset[\"Answer_text\"][sample_index]),\n",
        "            'answer_start' : [int(dataset[\"Answer_start\"][sample_index][1:-1])] if dataset[\"Answer_start\"][sample_index][1:-1] != '' else []\n",
        "        }\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"]) - 4\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_ikmbr_ExC"
      },
      "source": [
        "##### Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGxRXk_reb4h"
      },
      "outputs": [],
      "source": [
        "def grouped_LLRD_implementation(model, learning_rate):\n",
        "    opt_parameters = []\n",
        "    named_parameters = list(model.named_parameters()) \n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    set_2 = [\"layer.4\", \"layer.5\", \"layer.6\", \"layer.7\"]\n",
        "    set_3 = [\"layer.8\", \"layer.9\", \"layer.10\", \"layer.11\"]\n",
        "    init_lr = learning_rate\n",
        "    for i, (name, params) in enumerate(named_parameters):  \n",
        "        weight_decay = 0.0 if any(p in name for p in no_decay) else 0.01\n",
        "        if name.startswith(\"electra.embeddings\") or name.startswith(\"electra.encoder\"):            \n",
        "            # For first set, set lr to 1e-6 (i.e. 0.000001)\n",
        "            lr = init_lr       \n",
        "            # For set_2, increase lr to 0.00000175\n",
        "            lr = init_lr * 1.75 if any(p in name for p in set_2) else lr\n",
        "            \n",
        "            # For set_3, increase lr to 0.0000035 \n",
        "            lr = init_lr * 3.5 if any(p in name for p in set_3) else lr\n",
        "            \n",
        "            opt_parameters.append({\"params\": params,\n",
        "                                   \"weight_decay\": weight_decay,\n",
        "                                   \"lr\": lr})  \n",
        "            \n",
        "        # For regressor and pooler, set lr to 0.0000036 (slightly higher than the top layer).                \n",
        "        if name.startswith(\"regressor\") or name.startswith(\"roberta_model.pooler\"):               \n",
        "            lr = init_lr * 3.6 \n",
        "            \n",
        "            opt_parameters.append({\"params\": params,\n",
        "                                   \"weight_decay\": weight_decay,\n",
        "                                   \"lr\": lr})    \n",
        "    \n",
        "    return transformers.AdamW(opt_parameters, lr=init_lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_loop(model, tokenizer, pad_on_right, outfile, cluster = 0, learning_rate = 5e-6, epochs =3):\n",
        "    num_epochs = epochs\n",
        "    dataset = loaddataset(cluster)\n",
        "    #LLRD optimized AdamW\n",
        "    optimizer = grouped_LLRD_implementation(model, learning_rate)\n",
        "    tokenized_datasets = dataset.map(lambda x : preprocess_for_training(x, tokenizer, pad_on_right), batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "    train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator\n",
        "    )\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "    model.train()\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_loop = []\n",
        "        for batch in train_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            # print(loss)\n",
        "            optimizer.step()\n",
        "            # lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "            progress_bar.set_description(f'Epoch {epoch+1}/{epochs}')\n",
        "            loss_loop.append(loss.item())\n",
        "        print(f'Loss: {sum(loss_loop)/len(loss_loop)} after Epoch {epoch}')\n",
        "    model.save_pretrained(outfile)\n",
        "    tokenizer.save_pretrained(outfile)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4YoE7czutkr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer, pad_on_right = setup_device(model_name)"
      ],
      "metadata": {
        "id": "zuWJqSNSuMq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-1PSyNND4a4Q"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}