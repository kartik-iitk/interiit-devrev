{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vHtzxJq_pwHn",
        "M9jJuSCs26SW",
        "UzHw72nd6_wA",
        "BTEq0Khe7DM0",
        "JyKtVXm661x3",
        "Uxnd1xuWqVeA",
        "xccnbbAf7J1o",
        "raCs2XgbZpTG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions to Run:\n",
        "\n",
        "1. Upload sample_input_paragraph, question_answers, sample_input_question and sample_theme_interval csv.\n",
        "2. Execute helper functions cells for installing relevant libraries and importing them.\n",
        "3. Run the remaining cells as mentioned in the Sample_Eval notebook."
      ],
      "metadata": {
        "id": "KQKZfghyTQZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "vHtzxJq_pwHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load packages and import libraries"
      ],
      "metadata": {
        "id": "M9jJuSCs26SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!pip install -U sentence-transformers\n",
        "!pip install -U faiss-cpu\n",
        "!pip install transformers sentencepiece\n",
        "!pip install optimum[onnxruntime]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UjR7j2KkIIZ",
        "outputId": "07d44e0b-b1ce-4dbd-c3cb-2e00637bd38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=f53de17a6ae2073165110cde516c4c17a50280cdd62b23d7e8efefb2849db1b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.12.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optimum[onnxruntime]\n",
            "  Downloading optimum-1.6.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[onnxruntime]) (1.7.1)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from optimum[onnxruntime]) (4.26.0)\n",
            "Requirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.8/dist-packages (from optimum[onnxruntime]) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[onnxruntime]) (1.13.1+cu116)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from optimum[onnxruntime]) (0.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[onnxruntime]) (23.0)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.9.0\n",
            "  Downloading onnxruntime-1.13.1-cp38-cp38-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.2.1\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2023.1.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.8.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.64.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (3.9.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime>=1.9.0->optimum[onnxruntime]) (1.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.13.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.1.97)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[onnxruntime]) (1.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.2.1->optimum[onnxruntime]) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.2.1->optimum[onnxruntime]) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.2.1->optimum[onnxruntime]) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.2.1->optimum[onnxruntime]) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.2.1->optimum[onnxruntime]) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, protobuf, multiprocess, humanfriendly, onnx, coloredlogs, responses, onnxruntime, datasets, evaluate, optimum\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.18.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 datasets-2.9.0 evaluate-0.4.0 humanfriendly-10.0 multiprocess-0.70.14 onnx-1.12.0 onnxruntime-1.13.1 optimum-1.6.3 protobuf-3.20.1 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import gdown\n",
        "import nltk\n",
        "import faiss\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import timeit\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from optimum.onnxruntime import ORTModelForQuestionAnswering, ORTOptimizer\n",
        "from optimum.onnxruntime.configuration import OptimizationConfig\n",
        "from optimum.pipelines import pipeline\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPHDMY7Xkpfh",
        "outputId": "32fe8e3a-4543-43d0-d234-ccc2442eec4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzHw72nd6_wA"
      },
      "source": [
        "### Sentence Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FarPEbyBsIh"
      },
      "source": [
        "For a given theme, break its paragraphs into sentences and store their paragraph id. Load sentence encoder and calculate embeddings for the sentences from paragraphs and the queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-iuMv8R7BPv"
      },
      "outputs": [],
      "source": [
        "def para_to_sentences(para):\n",
        "    \"\"\"Splits a paragraph into sentences.\"\"\"\n",
        "    para = para.replace('\\n', ' ').replace('\\t', ' ').replace('\\x00', ' ')\n",
        "    return nltk.sent_tokenize(para)\n",
        "\n",
        "def load_sents_from_para(paras):\n",
        "    \"\"\"Splits a list of paragraphs into sentences and returns the sentences\n",
        "    and their corresponding paragraph id\"\"\"\n",
        "    sents = []\n",
        "    para_id = []\n",
        "    for i,p in enumerate(paras):\n",
        "        new_sents = para_to_sentences(p['paragraph'])\n",
        "        sents += new_sents\n",
        "        para_id += [p['id']]*len(new_sents)\n",
        "    return sents, para_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQKHrVVbDvwN"
      },
      "outputs": [],
      "source": [
        "def load_encoder():\n",
        "    \"\"\"Load mpnet-base-v2 Sentence Encoder\"\"\"\n",
        "    # model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/file/d/137tZvp-iTMR2xIogasglSH4jTTLW4_Sf/view\",\n",
        "        fuzzy=True, use_cookies=False, quiet=True\n",
        "    )\n",
        "    with ZipFile('/content/finetuned_mpnet_triplet.zip') as zobj:\n",
        "        zobj.extractall()\n",
        "    model = SentenceTransformer('/content/kaggle/working/finetuned_mpnet_triplet')\n",
        "    return model\n",
        "\n",
        "def get_embeddings(sents, model):\n",
        "    \"\"\"Generates embeddings for each sentence in the list of 768 dimesions\"\"\"\n",
        "    return model.encode(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTEq0Khe7DM0"
      },
      "source": [
        "### Nearest Neighbour Search using FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo6xW9HsHoMl"
      },
      "source": [
        "Based on the embeddings calculated, indexes them based on L2 distance and then applies nearest neighbour search to get top k closest sentences for each query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SwA5Hhc7JMO"
      },
      "outputs": [],
      "source": [
        "def save_index(source_embeds, output_path):\n",
        "    \"\"\"Creates and saves the faiss L2 Index using source_embeds\"\"\"\n",
        "    index = faiss.IndexFlatL2(source_embeds.shape[1])\n",
        "    index.add(np.array(source_embeds))\n",
        "    faiss.write_index(index, output_path)\n",
        "\n",
        "def load_index(path):\n",
        "    \"\"\"Loads faiss index from the disk\"\"\"\n",
        "    index = faiss.read_index(path)\n",
        "    return index\n",
        "\n",
        "def get_k_nearest_neighbours(index, query_embeds, k = 10):\n",
        "    \"\"\"Returns k nearest neighbours of target_embeds in source_embeds\"\"\"\n",
        "    return index.search(np.array(query_embeds), k)\n",
        "\n",
        "def get_nearest_queries(ques_embed, theme):\n",
        "    \"\"\"Retrieve nearest already answered queries to the questions\"\"\"\n",
        "    index = load_index(f'/content/indices/{theme}_ques_l2_index')\n",
        "    return get_k_nearest_neighbours(index, ques_embed, 3)\n",
        "\n",
        "def get_nearest_sentences(ques_embed, theme):\n",
        "    \"\"\"Retrieve nearest sentences to the questions\"\"\"\n",
        "    index = load_index(f'/content/indices/{theme}_para_l2_index')\n",
        "    return get_k_nearest_neighbours(index, ques_embed, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyKtVXm661x3"
      },
      "source": [
        "### Load Existing QA and paragraphs data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdk3n4f899fc"
      },
      "source": [
        "Load validation data for testing, based on missing data in the training data from squad 2.0 dataset. Round 1 data contains themes that are not present in training data. While, round 2 data contains themes that are present in training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDsZ3veewUjM"
      },
      "outputs": [],
      "source": [
        "def load_existing_data():\n",
        "    \"\"\"Load already answered questions and paragraphs, theme-wise.\n",
        "    Also breaks the paragraphs into sentences\"\"\"\n",
        "    paras, solved_ques = {}, {}\n",
        "    paragraphs = json.loads(pd.read_csv(\"sample_input_paragraph.csv\").to_json(orient=\"records\"))\n",
        "    questions = json.loads(pd.read_csv(\"question_answers.csv\").to_json(orient=\"records\"))\n",
        "    theme_intervals = json.loads(pd.read_csv(\"sample_theme_interval.csv\").to_json(orient=\"records\"))\n",
        "    \n",
        "    for theme_interval in theme_intervals:\n",
        "        theme = theme_interval[\"theme\"]\n",
        "        theme_paras = [p for p in paragraphs if p[\"theme\"] == theme]\n",
        "        sents, para_id = load_sents_from_para(theme_paras)\n",
        "        paras[theme] = {\n",
        "            'id': para_id,\n",
        "            'sentences': sents\n",
        "        }\n",
        "    \n",
        "    for i, ques in enumerate(questions):\n",
        "        theme = ques['theme']\n",
        "        if theme not in solved_ques:\n",
        "            solved_ques[theme] = {\n",
        "                'id': [],\n",
        "                'question': [],\n",
        "                'paragraph_id': [],\n",
        "                'answers': []\n",
        "            }\n",
        "        solved_ques[theme]['id'].append(i)\n",
        "        solved_ques[theme]['question'].append(ques[\"question\"])\n",
        "        solved_ques[theme]['paragraph_id'].append(ques[\"paragraph_id\"])\n",
        "        solved_ques[theme]['answers'].append(ques[\"answer\"])\n",
        "    return paras, solved_ques\n",
        "\n",
        "\n",
        "def store_faiss_indices(paras, solved_ques, encoder):\n",
        "    \"\"\"Generates embeddings for paragraph sentences and queries. Then it creates\n",
        "    and saves the faiss index using them into disk\"\"\"\n",
        "    if not os.path.exists('/content/indices/'):\n",
        "        os.mkdir('/content/indices/')\n",
        "    for theme in paras:\n",
        "        theme_paras = paras[theme]\n",
        "        \n",
        "        output_path = f'/content/indices/{theme}_para_l2_index'\n",
        "        if not os.path.exists(output_path):\n",
        "            para_embeds = get_embeddings(theme_paras['sentences'], encoder)\n",
        "            save_index(para_embeds, output_path)\n",
        "        \n",
        "        output_path = f'/content/indices/{theme}_ques_l2_index'    \n",
        "        if theme in solved_ques and not os.path.exists(output_path):\n",
        "            theme_ques = solved_ques[theme]\n",
        "            ques_embeds = get_embeddings(theme_ques['question'], encoder)\n",
        "            save_index(ques_embeds, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxnd1xuWqVeA"
      },
      "source": [
        "### Search previously answered queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSpVs8P8qoMh"
      },
      "outputs": [],
      "source": [
        "def search_previously_answered_queries(q_id, dist, query_idx, solved_queries):\n",
        "    \"\"\"Search previously answered queries and return its answer if it exists\"\"\"\n",
        "    if dist > query_threshold:\n",
        "        return False, None\n",
        "    ans = {\n",
        "        \"question_id\": q_id,\n",
        "        \"answers\": solved_queries['answers'][query_idx],\n",
        "        \"paragraph_id\": solved_queries['paragraph_id'][query_idx]\n",
        "    }\n",
        "    return True, ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xccnbbAf7J1o"
      },
      "source": [
        "### Context Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJgYZNGdR5ni"
      },
      "source": [
        "Generates a context for a given query and its nearest neighbours. Also provides a method to get the paragraph id given the start idx of the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdyftt2i7SMV"
      },
      "outputs": [],
      "source": [
        "def get_context(sents, para_ids, nearest_neighbours, distances):\n",
        "    \"\"\"Generate the context for a given query and store the para_id for\n",
        "    each sentence\"\"\"\n",
        "    context = \"\"\n",
        "    context_para_ids, sent_length = [], []\n",
        "    for sent_id, dist in zip(nearest_neighbours, distances):\n",
        "        if dist > distance_threshold*distances[0]:\n",
        "            break\n",
        "        context += sents[sent_id] + ' '\n",
        "        context_para_ids.append(para_ids[sent_id])\n",
        "        sent_length.append(len(sents[sent_id]))\n",
        "        if len(context.split()) >= context_length_threshold:\n",
        "            break\n",
        "    sum = -1\n",
        "    for i in range(len(sent_length)):\n",
        "        sum += sent_length[i] + 1\n",
        "        sent_length[i] = sum\n",
        "    return context.strip(), context_para_ids, sent_length\n",
        "\n",
        "\n",
        "def para_id_retriever(start_idx, sent_length, context_para_ids):\n",
        "    \"\"\"Given start index of the answer, return the id of the paragraph\n",
        "    in which the answer belongs\"\"\"\n",
        "    if start_idx == -1:\n",
        "        return -1\n",
        "    for j in range(len(sent_length)):\n",
        "        if start_idx <= sent_length[j]:\n",
        "            return context_para_ids[j]\n",
        "    return context_para_ids[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raCs2XgbZpTG"
      },
      "source": [
        "### Load fine-tuned QA models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actaNQ_CafqR"
      },
      "source": [
        "Given a theme, load the corresponding fine-tuned QA model and load the QA pipeline "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_fine_tuned_models():\n",
        "    \"\"\"Download and unzip cluster-wise fine-tuned QA models\"\"\"\n",
        "    urls = [\n",
        "        (\"1-7XfPhjfmUo8xz0iqmFHusbZ74q-SS3A\", \"zipped_0_11.tar.gz\"),\n",
        "        (\"1-BIhfqK992YZW1eWiG5yOCLiX8vOyrNI\", \"zipped_12_22.tar.gz\"),\n",
        "        (\"1-B8b2_s9i2pwTn7EPgzMg50nNM6Dp4B-\", \"zipped_23_34.tar.gz\"),\n",
        "        (\"1-KDxa6wWMGqrDR7ZJq-bYSyWaa_Zsikq\", \"zipped_35_42.tar.gz\")\n",
        "    ]\n",
        "    for url, filename in urls:\n",
        "        if not os.path.exists(filename):\n",
        "            link = f\"https://drive.google.com/u/1/uc?id={url}&export=download\"\n",
        "            gdown.download(link, quiet=True, use_cookies=False)\n",
        "            with tarfile.open(filename, 'r') as tar:\n",
        "                tar.extractall()\n",
        "            # os.remove(filename)\n",
        "\n",
        "def download_generic_model():\n",
        "    \"\"\"Download and optimize electra base model using onnx\"\"\"\n",
        "    model_id = 'PremalMatalia/electra-base-best-squad2'\n",
        "    save_path = \"/content/models/generic_model/\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    ort_model = ORTModelForQuestionAnswering.from_pretrained(\n",
        "        model_id, from_transformers=True\n",
        "    )\n",
        "    optimizer = ORTOptimizer.from_pretrained(ort_model)\n",
        "    optimization_config = OptimizationConfig(optimization_level=99)\n",
        "    optimizer.optimize(save_dir=save_path, optimization_config=optimization_config)"
      ],
      "metadata": {
        "id": "uN7wGiaRuUn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNzi6Bz-mGED"
      },
      "outputs": [],
      "source": [
        "def load_models_mapping():\n",
        "    \"\"\"Loads map for checking cluster of a theme and vice versa\"\"\"\n",
        "    theme_to_cluster = {}\n",
        "    cluster_to_themes = {}\n",
        "    if not os.path.exists(\"clusters.json\"):\n",
        "        file_url = \"https://drive.google.com/file/d/1P6dp7f2m67-iPaUbaNZiDYTmTH7Mw9ec/view?usp=share_link\"\n",
        "        gdown.download(url=file_url, output='clusters.json', quiet=False, fuzzy=True)\n",
        "    with open('clusters.json') as fo:\n",
        "        map = json.load(fo)\n",
        "    for cluster, themes in map.items():\n",
        "        cluster = int(cluster)\n",
        "        if cluster not in cluster_to_themes:\n",
        "            cluster_to_themes[cluster] = []\n",
        "        for theme in themes:\n",
        "            theme_to_cluster[theme] = cluster\n",
        "            cluster_to_themes[cluster].append(theme)\n",
        "    return theme_to_cluster, cluster_to_themes\n",
        "\n",
        "\n",
        "def load_qa_model_pipeline(model_path):\n",
        "    \"\"\"Load QA model pipeline for a given cluster\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    for i in range(5):\n",
        "        try:\n",
        "            model = ORTModelForQuestionAnswering.from_pretrained(\n",
        "                model_path, file_name=\"model_optimized.onnx\"\n",
        "            )\n",
        "        except:\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "    optimum_qa = pipeline(\n",
        "        task = 'question-answering', model=model,\n",
        "        tokenizer=tokenizer, handle_impossible_answer=True\n",
        "    )\n",
        "    return optimum_qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q_1CZAKZZ1w"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are not using fine-tuned QA models as fine-tuning on one cluster\n",
        "# takes 3 hours on CPU. So, we could not train for all 43 clusters\n",
        "# in under 12 hours."
      ],
      "metadata": {
        "id": "rrDTD4ZlF681"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sentence encoder model and fine-tuned QA models\n",
        "# download_fine_tuned_models()\n",
        "download_generic_model()\n",
        "sentence_encoder = load_encoder()\n",
        "theme_to_cluster, cluster_to_themes = load_models_mapping()\n",
        "\n",
        "# Load existing QA pairs for themes and pre-process it\n",
        "paras, solved_ques = load_existing_data()\n",
        "store_faiss_indices(paras, solved_ques, sentence_encoder)"
      ],
      "metadata": {
        "id": "YWnuk1j6qPuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for context generation\n",
        "k = 10\n",
        "query_threshold = 0.2\n",
        "distance_threshold = 2\n",
        "context_length_threshold = 205"
      ],
      "metadata": {
        "id": "W0RjrWvFoIRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_theme_model(theme):\n",
        "    \"\"\"Load theme model if available, otherwise use generic model\"\"\"\n",
        "    if theme in theme_to_cluster:\n",
        "        cluster = theme_to_cluster[theme]\n",
        "        model_path = f'/content/models/electra-base-best-squad2-finetuned-squad-{cluster}'\n",
        "        if os.path.exists(model_path):\n",
        "            return load_qa_model_pipeline(model_path)\n",
        "    model_path = f'/content/models/generic_model'\n",
        "    return load_qa_model_pipeline(model_path)"
      ],
      "metadata": {
        "id": "p8RcyC4O2DFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_theme_ans(questions, theme_model, pred_out):\n",
        "    ann_inference_time, qna_inference_time = 0., 0.\n",
        "    theme = questions[0][\"theme\"]\n",
        "    solved_queries_exists = False\n",
        "    if theme in solved_ques:\n",
        "        solved_queries = solved_ques[theme]\n",
        "        solved_queries_exists = True\n",
        "    print(f'Theme: {theme}')\n",
        "\n",
        "    # Nearest Neighbour Search\n",
        "    start_time = time.time()\n",
        "    ques_list = [q['question'] for q in questions]\n",
        "    ques_embed = get_embeddings(ques_list, sentence_encoder)\n",
        "    if solved_queries_exists:\n",
        "        D_ques, I_ques = get_nearest_queries(ques_embed, theme)\n",
        "    D_sents, I_sents = get_nearest_sentences(ques_embed, theme)\n",
        "    ann_inference_time = (time.time() - start_time)*1000.\n",
        "\n",
        "    # QA Model Prediction\n",
        "    start_time = time.time()\n",
        "    for i in tqdm(range(len(questions))):\n",
        "        q = questions[i]\n",
        "        # Check previously answered queries\n",
        "        if solved_queries_exists:\n",
        "            found, ans = search_previously_answered_queries(\n",
        "                q[\"id\"], D_ques[i,0], I_ques[i,0], solved_queries\n",
        "            )\n",
        "            if found:\n",
        "                pred_out.append(ans)\n",
        "                continue\n",
        "\n",
        "        # Context Generation\n",
        "        context, context_para_ids, sent_length = get_context(\n",
        "            paras[theme]['sentences'], paras[theme]['id'], I_sents[i], D_sents[i]\n",
        "        )\n",
        "        # Answer Prediction and Paragraph Retrieval\n",
        "        prediction = theme_model(question=q['question'], context=context)\n",
        "        ans = {\n",
        "            \"question_id\": q['id'],\n",
        "            \"answers\": prediction['answer'],\n",
        "            \"paragraph_id\": -1\n",
        "        }\n",
        "        if prediction['answer'] != \"\":\n",
        "            ans[\"paragraph_id\"] = para_id_retriever(\n",
        "                prediction['start'], sent_length, context_para_ids\n",
        "            )\n",
        "        pred_out.append(ans)\n",
        "\n",
        "    # Print Inference Time\n",
        "    qna_inference_time = (time.time() - start_time)*1000.\n",
        "    print(\n",
        "        f'Avg. ANN IT = {round(ann_inference_time/len(questions), 2)} ms, ' +\n",
        "        f'Avg. QnA IT = {round(qna_inference_time/len(questions),2)} ms\\n'\n",
        "    )"
      ],
      "metadata": {
        "id": "Aa4x0ljoIGpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes. \n",
        "\n",
        "# All theme prediction.\n",
        "questions = json.loads(pd.read_csv(\"sample_input_question.csv\").to_json(orient=\"records\"))\n",
        "theme_intervals = json.loads(pd.read_csv(\"sample_theme_interval.csv\").to_json(orient=\"records\"))\n",
        "pred_out = []\n",
        "theme_inf_time = {}\n",
        "for theme_interval in theme_intervals:\n",
        "    theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n",
        "    theme = theme_ques[0][\"theme\"]\n",
        "    # Load model fine-tuned for this theme.\n",
        "    theme_model = get_theme_model(theme)\n",
        "    execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n",
        "    theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n",
        "pred_df = pd.DataFrame.from_records(pred_out)\n",
        "pred_df.fillna(value='', inplace=True)\n",
        "# Write prediction to a CSV file. Teams are required to submit this csv file.\n",
        "pred_df.to_csv('output_prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "V4MGougaIImX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13d43ad-9431-4a9d-e425-91adc15552ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theme: IPod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 222/222 [00:07<00:00, 28.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 59.25 ms, Avg. QnA IT = 34.54 ms\n",
            "\n",
            "Theme: 2008_Sichuan_earthquake\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 192/192 [01:08<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 45.06 ms, Avg. QnA IT = 354.49 ms\n",
            "\n",
            "Theme: Wayback_Machine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:01<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 61.12 ms, Avg. QnA IT = 758.44 ms\n",
            "\n",
            "Theme: Canadian_Armed_Forces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 133/133 [01:23<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 52.69 ms, Avg. QnA IT = 626.6 ms\n",
            "\n",
            "Theme: Cardinal_(Catholicism)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [01:24<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 54.72 ms, Avg. QnA IT = 772.69 ms\n",
            "\n",
            "Theme: Human_Development_Index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 57/57 [00:28<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 58.73 ms, Avg. QnA IT = 495.55 ms\n",
            "\n",
            "Theme: Heresy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:52<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 53.53 ms, Avg. QnA IT = 768.17 ms\n",
            "\n",
            "Theme: Warsaw_Pact\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [00:37<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 53.72 ms, Avg. QnA IT = 807.13 ms\n",
            "\n",
            "Theme: Materialism\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:53<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 62.13 ms, Avg. QnA IT = 791.61 ms\n",
            "\n",
            "Theme: Pub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102/102 [00:36<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 69.53 ms, Avg. QnA IT = 355.33 ms\n",
            "\n",
            "Theme: Web_browser\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66/66 [00:44<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 69.52 ms, Avg. QnA IT = 676.55 ms\n",
            "\n",
            "Theme: Catalan_language\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [00:48<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 42.21 ms, Avg. QnA IT = 440.4 ms\n",
            "\n",
            "Theme: Paper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 117/117 [01:09<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 63.02 ms, Avg. QnA IT = 590.93 ms\n",
            "\n",
            "Theme: Adult_contemporary_music\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 73/73 [00:32<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 83.14 ms, Avg. QnA IT = 445.0 ms\n",
            "\n",
            "Theme: Nanjing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 206/206 [02:45<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 55.1 ms, Avg. QnA IT = 805.52 ms\n",
            "\n",
            "Theme: Dialect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 251/251 [03:19<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 66.96 ms, Avg. QnA IT = 793.93 ms\n",
            "\n",
            "Theme: Southampton\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 318/318 [03:43<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 60.45 ms, Avg. QnA IT = 701.35 ms\n",
            "\n",
            "Theme: The_Times\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 141/141 [01:52<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 80.39 ms, Avg. QnA IT = 796.83 ms\n",
            "\n",
            "Theme: Immunology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:53<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 50.56 ms, Avg. QnA IT = 882.69 ms\n",
            "\n",
            "Theme: Imamah_(Shia_doctrine)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:42<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 68.92 ms, Avg. QnA IT = 889.05 ms\n",
            "\n",
            "Theme: Grape\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:26<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 77.58 ms, Avg. QnA IT = 758.1 ms\n",
            "\n",
            "Theme: United_States_dollar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 235/235 [03:21<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 50.92 ms, Avg. QnA IT = 858.71 ms\n",
            "\n",
            "Theme: Everton_F.C.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158/158 [01:57<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 62.92 ms, Avg. QnA IT = 741.49 ms\n",
            "\n",
            "Theme: Hard_rock\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 178/178 [02:46<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 56.31 ms, Avg. QnA IT = 936.52 ms\n",
            "\n",
            "Theme: Great_Plains\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:56<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 50.74 ms, Avg. QnA IT = 748.93 ms\n",
            "\n",
            "Theme: Biodiversity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 194/194 [02:12<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 53.86 ms, Avg. QnA IT = 681.35 ms\n",
            "\n",
            "Theme: Federal_Bureau_of_Investigation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 304/304 [05:31<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 52.56 ms, Avg. QnA IT = 1091.24 ms\n",
            "\n",
            "Theme: Mary_(mother_of_Jesus)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 247/247 [04:24<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 57.6 ms, Avg. QnA IT = 1072.16 ms\n",
            "\n",
            "Theme: Unknown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:02<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 63.11 ms, Avg. QnA IT = 272.57 ms\n",
            "\n",
            "Theme: DevRev\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:03<00:00,  2.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. ANN IT = 38.15 ms, Avg. QnA IT = 423.82 ms\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_inf_time = 0.0\n",
        "total_queries = 0\n",
        "for theme_interval in theme_intervals:\n",
        "    num_queries = int(theme_interval[\"end\"]) - int(theme_interval[\"start\"]) + 1\n",
        "    exec_time = theme_inf_time[theme_interval[\"theme\"]]\n",
        "    total_queries += num_queries\n",
        "    total_inf_time += exec_time\n",
        "print(f'Average Execution Time: {total_inf_time / total_queries} ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bi_aZUtfZ11",
        "outputId": "7b52a957-0ac0-4f19-e2c8-4504d1b4b94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Execution Time: 775.6525714352819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDmivKd7JS4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}