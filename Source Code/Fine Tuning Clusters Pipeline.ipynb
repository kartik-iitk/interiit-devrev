{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1PSyNND4a4Q"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcBT6tLgYwwm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install -U transformers\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh30HOkS4mC0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TrainingArguments, Trainer, AutoTokenizer, BertTokenizerFast, BertForQuestionAnswering, AutoModelForQuestionAnswering\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_scheduler\n",
        "from transformers import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0r8paULkIC"
      },
      "source": [
        "####  Data Loading."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading cluster-wise csvs\n",
        "%%capture\n",
        "! gdown 1JaMYQrbtpREDw7E8VUqtNAPSod_eP9u2\n",
        "! unzip datasets.zip"
      ],
      "metadata": {
        "id": "Cjwf_L3liuMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mr4nTU4dpyu"
      },
      "outputs": [],
      "source": [
        "def loaddataset(cluster = 0):\n",
        "  base_url = 'datasets/'\n",
        "  data_files = {\"train\":base_url + \"{}_train.csv\".format(cluster), \"test\": base_url + \"{}_test.csv\".format(cluster)}\n",
        "  # data_files = {\"train\":base_url + \"{}_train.csv\".format(theme), \"test\": base_url + \"{}_test.csv\".format(theme)}\n",
        "  dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNrvZPOZ6Bqn"
      },
      "source": [
        "\n",
        "#### Finetuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQemcukMPZz"
      },
      "source": [
        "##### Device Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YOQzlfiMjEO"
      },
      "outputs": [],
      "source": [
        "#setting up global variables\n",
        "model_name = \"PremalMatalia/electra-base-best-squad2\"\n",
        "batch_size = 8\n",
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_device(model_name):\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast), \"Fast Tokenizer Needed\"\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "  pad_on_right = tokenizer.padding_side == \"right\"\n",
        "  model.to(device)\n",
        "  return model, tokenizer, pad_on_right\n"
      ],
      "metadata": {
        "id": "rxsKYTYBmAsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ZX77CuxDxr"
      },
      "source": [
        "\n",
        "##### Preprocessing for FineTuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwEoL09kxBcC"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_training(dataset, tokenizer, pad_on_right):\n",
        "    dataset[\"Question\"] = [q.lstrip() for q in dataset[\"Question\"]]\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        dataset[\"Question\" if pad_on_right else \"Paragraph\"],\n",
        "        dataset[\"Paragraph\" if pad_on_right else \"Qustion\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "    # print(sample_mapping, offset_mapping)\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        #! WORKS\n",
        "\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = {\n",
        "            'text' : (dataset[\"Answer_text\"][sample_index]),\n",
        "            'answer_start' : [int(dataset[\"Answer_start\"][sample_index][1:-1])] if dataset[\"Answer_start\"][sample_index][1:-1] != '' else []\n",
        "        }\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"]) - 4\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_ikmbr_ExC"
      },
      "source": [
        "##### Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8YOZf1HmaMq"
      },
      "outputs": [],
      "source": [
        "def fine_tune_using_api(model, tokenizer, pad_on_right, cluster = 0, learning_rate = 7e-7, epochs = 4, scheduler = 'linear'):\n",
        "  dataset = loaddataset(cluster)\n",
        "  #Training Args\n",
        "  model_name_ = model_name.split(\"/\")[-1]\n",
        "  args = TrainingArguments(\n",
        "      f\"{model_name_}-finetuned-squad-{cluster}-trainedfor-{epochs}\",\n",
        "      evaluation_strategy = \"epoch\",\n",
        "      learning_rate=learning_rate,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      per_device_eval_batch_size=batch_size,\n",
        "      lr_scheduler_type = \"linear\",\n",
        "      optim = \"adamw_hf\",\n",
        "      # warmup_proportion = 0.2,\n",
        "      num_train_epochs=epochs,\n",
        "      weight_decay=0.01,\n",
        "      save_strategy = \"no\",\n",
        "      push_to_hub=False, #because why not\n",
        "  )\n",
        "  tokenized_datasets = dataset.map(lambda x : preprocess_for_training(x, tokenizer, pad_on_right), batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "  trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "  )\n",
        "  trainer.train()\n",
        "  # login_please()\n",
        "  trainer.save_model(f\"{model_name_}-finetuned-squad-{cluster}\")\n",
        "  return model #interim, do not want to refactor all my code lmaooo"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-1PSyNND4a4Q",
        "Av0r8paULkIC",
        "hbQemcukMPZz",
        "X7ZX77CuxDxr"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}