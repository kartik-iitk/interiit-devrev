{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w-ETDXir9ues",
        "W597h23gXlG5",
        "Gd1RGk1u7fpo",
        "4r2-FHHi8qYr",
        "jYScnopC70qb",
        "vykT60ECY3IP",
        "gTJHEIhdBC2A",
        "9RIiPeYgGkYP",
        "Ttxi4-5u5jXL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive\n",
        "\n",
        "Use:\n",
        "* Saving the output file.\n",
        "* Quicker loading of the heavy SynQA Model in between subsequent runs."
      ],
      "metadata": {
        "id": "w-ETDXir9ues"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "SjJl58V59xfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Imports\n",
        "\n",
        "Download all the required models and perform necessary imports for the models."
      ],
      "metadata": {
        "id": "W597h23gXlG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KeyBert with KeyPhrase-Vectorizers\n",
        "\n",
        "Model used for Answer Generation (using NER) for synQA Model."
      ],
      "metadata": {
        "id": "Gd1RGk1u7fpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keyphrase-vectorizers\n",
        "!pip install keybert"
      ],
      "metadata": {
        "id": "Sq-8AlxZ6S3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer"
      ],
      "metadata": {
        "id": "zTpNXjqi4BGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### synQA-question-generators\n",
        "\n",
        "Answer Aware model for Question Generation."
      ],
      "metadata": {
        "id": "XJ400zWI9GbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install fairseq"
      ],
      "metadata": {
        "id": "N2iDj3LY9ddT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from fairseq.models.transformer import TransformerModel"
      ],
      "metadata": {
        "id": "5pGMVbiD9eQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_synQA(url: str, fname: str, desc: str = None) -> None:\n",
        "    desc = desc if desc is not None else fname\n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get(\"content-length\", 0))\n",
        "    with open(fname, \"wb\") as file, tqdm(\n",
        "        desc=fname, total=total, unit=\"iB\", unit_scale=True, unit_divisor=1024\n",
        "    ) as bar:\n",
        "        for data in resp.iter_content(chunk_size=1024):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)"
      ],
      "metadata": {
        "id": "zkRh-t6g-DSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_synQA(MODEL_URL, MODELS_DIR):\n",
        "    for model_filename, url in MODEL_URL.items():\n",
        "        model_name = model_filename.split(\".\")[0]\n",
        "        model_tarfile_path = os.path.join(MODELS_DIR, model_filename)\n",
        "        model_dir = os.path.join(MODELS_DIR, model_name)\n",
        "\n",
        "        if not os.path.exists(os.path.join(model_dir, \"checkpoint_best.pt\")):\n",
        "            if not os.path.exists(model_tarfile_path):\n",
        "                download_synQA(url, model_tarfile_path, url)\n",
        "\n",
        "            # Extracting {model_filename} to {model_dir}\n",
        "            with tarfile.open(model_tarfile_path) as f:\n",
        "                # Get only the members with extensions (i.e. no directories)\n",
        "                members = [\n",
        "                    m\n",
        "                    for m in f.getmembers()\n",
        "                    if os.path.splitext(os.path.join(model_dir, m.name))[-1]\n",
        "                ]\n",
        "                # Flatten (i.e. remove directory info)\n",
        "                for m in members:\n",
        "                    m.name = os.path.basename(m.name)\n",
        "                # Extract\n",
        "                def is_within_directory(directory, target):\n",
        "                    abs_directory = os.path.abspath(directory)\n",
        "                    abs_target = os.path.abspath(target)\n",
        "                    prefix = os.path.commonprefix([abs_directory, abs_target])\n",
        "                    return prefix == abs_directory\n",
        "\n",
        "                def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n",
        "                    for member in tar.getmembers():\n",
        "                        member_path = os.path.join(path, member.name)\n",
        "                        if not is_within_directory(path, member_path):\n",
        "                            raise Exception(\"Attempted Path Traversal in Tar File\")\n",
        "                    tar.extractall(path, members, numeric_owner=numeric_owner)\n",
        "\n",
        "                safe_extract(f, model_dir, members=members)\n",
        "\n",
        "            # Remove tarfile\n",
        "            os.remove(model_tarfile_path)"
      ],
      "metadata": {
        "id": "LK8UugA--FoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t5_small\n",
        "\n",
        "Answer Agnostic model for generating Question-Answer pairs."
      ],
      "metadata": {
        "id": "4r2-FHHi8qYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!python -m nltk.downloader punkt\n",
        "!git clone https://github.com/patil-suraj/question_generation.git"
      ],
      "metadata": {
        "id": "HKLH0Ory8y_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd question_generation\n",
        "from pipelines import pipeline"
      ],
      "metadata": {
        "id": "LhokYFJGCgYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy\n",
        "\n",
        "NER Model which is used to decide which model is to be used for Question Generation."
      ],
      "metadata": {
        "id": "jYScnopC70qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.symbols import *\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "L_SYpd_Q75U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other imports"
      ],
      "metadata": {
        "id": "vykT60ECY3IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8SPgJGAoZRtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Test Data"
      ],
      "metadata": {
        "id": "gTJHEIhdBC2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1RAD2mJbz4yQddZZaUvXDWQC6Okar-QVh"
      ],
      "metadata": {
        "id": "Dv-nQ19TBGEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_to_theme_wise(test_paragraphs):\n",
        "    theme_wise_para = {}\n",
        "    for item in test_paragraphs:\n",
        "        if item['theme'] not in theme_wise_para.keys():\n",
        "            theme_wise_para[item['theme']] = [[item['id'],item['paragraph']],]\n",
        "        else:\n",
        "            theme_wise_para[item['theme']].append([item['id'],item['paragraph']])\n",
        "    return theme_wise_para"
      ],
      "metadata": {
        "id": "QJ9DP3Hmtar9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question-Answer Generator Functions"
      ],
      "metadata": {
        "id": "9RIiPeYgGkYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ans_phrases(kw_model, context, top_n=10, use_maxsum=True, diversity=0.2, nr_candidates=20):\n",
        "    contexts = [context]\n",
        "    keyphr = kw_model.extract_keywords(docs=contexts, vectorizer=KeyphraseCountVectorizer(), top_n = top_n, use_maxsum=True, diversity=0.2, nr_candidates=20)\n",
        "    keybert_phrases=[]\n",
        "    for i in range(len(keyphr)):\n",
        "        keybert_phrases.append(keyphr[i][0])\n",
        "    return keybert_phrases"
      ],
      "metadata": {
        "id": "1HHknDTvJ3vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_example_to_input(example):\n",
        "    ex_input_inner = f\" {SPECIAL_TOKENS['sep_token']} \".join(example)\n",
        "    ex_input = (\n",
        "        f\"{SPECIAL_TOKENS['bos_token']} {ex_input_inner} {SPECIAL_TOKENS['eos_token']}\"\n",
        "    )\n",
        "    return ex_input"
      ],
      "metadata": {
        "id": "VjY4JseBba7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_special_tokens(text):\n",
        "    for _, special_tok in SPECIAL_TOKENS.items():\n",
        "        text = text.replace(special_tok, \"\")\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "t1y_QCSmblue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_using_synQA(context, generator, answer_phrases, num_questions):\n",
        "    s_ans = []\n",
        "    s_ques = []\n",
        "    for i in range(min(len(answer_phrases), num_questions)):\n",
        "        # if answer_phrases[i] not in context:\n",
        "        #     print(f\"The answer provided ({answer_phrases[i]}) is not in the context.\")\n",
        "        example = [answer_phrases[i], context]\n",
        "        ex_input = convert_example_to_input(example)\n",
        "        ex_inputs = [ex_input]\n",
        "        for _ in range(1):\n",
        "            output = generator.translate(ex_inputs, **decode_params)\n",
        "            if isinstance(output, str):\n",
        "                clean_output = clean_special_tokens(output)\n",
        "            else:\n",
        "                clean_output = [clean_special_tokens(q) for q in output]\n",
        "                if len(clean_output) == 1:\n",
        "                    clean_output = clean_output[0]\n",
        "            s_ans.append(answer_phrases[i])\n",
        "            s_ques.append(clean_output)\n",
        "    return s_ques, s_ans"
      ],
      "metadata": {
        "id": "kTt-cii1GdMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_using_t5(context, nlp):\n",
        "    s_ques = []\n",
        "    s_ans = []\n",
        "    result = nlp(context)\n",
        "    for i in result:\n",
        "        s_ques.append(i['question'])\n",
        "        s_ans.append(i['answer'][6:]) # Remove the initial <pad> tag.\n",
        "    return s_ques, s_ans"
      ],
      "metadata": {
        "id": "BL2kDsq-ODZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_start(para, answer):\n",
        "    para = para.lower()\n",
        "    answer = answer.lower()\n",
        "    return (para.find(answer) + 1)"
      ],
      "metadata": {
        "id": "LiZ8n2UPnUeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qa(input_data, spc, nlp, kw_model, kw_args, generator):\n",
        "    qid = 1     # Used for giving id to each question.\n",
        "    output = []\n",
        "    for theme in input_data:\n",
        "        obj = input_data[theme]\n",
        "        for id, para in obj:\n",
        "            phrase_extraction = spc(para)\n",
        "            if len(phrase_extraction.ents) == 0:\n",
        "                answer_phrases = get_ans_phrases(kw_model, para, top_n=kw_args['top_n'], use_maxsum=kw_args['use_maxsum'], diversity=kw_args['diversity'], nr_candidates=kw_args['nr_candidates'])\n",
        "                ques, ans = generate_using_synQA(para, generator, answer_phrases, num_questions = 5)\n",
        "            else:\n",
        "                ques, ans = generate_using_t5(para, nlp)\n",
        "            for i in zip(ques, ans):\n",
        "                row = [qid, theme, para, i[0], 'TRUE', [i[1]], [get_answer_start(para, i[1])]]        \n",
        "                output.append(row)\n",
        "                qid += 1\n",
        "    df = pd.DataFrame(output, columns=['id', 'Theme', 'Paragraph', 'Question', 'Answer_possible', 'Answer_text', 'Answer_start'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "keyx2tK9QCft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "qbFtocf96MbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load all Models\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "MODELS_DIR = \"/content/gdrive/MyDrive/Colab Notebooks/synthetic_data/models\"\n",
        "MODEL_URL = {\"generator_qa_squad_plus_adversarialqa.tgz\": \"https://dl.fbaipublicfiles.com/dynabench/qa/qgen_dcombined_plus_squad_10k.tgz\"}\n",
        "MODEL_NAME = 'generator_qa_squad_plus_adversarialqa'\n",
        "MODEL_PATH = os.path.join(MODELS_DIR, MODEL_NAME)\n",
        "download_and_extract_synQA(MODEL_URL, MODELS_DIR)\n",
        "\n",
        "generator = TransformerModel.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    checkpoint_file='checkpoint_best.pt',\n",
        "    bpe='gpt2',\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "nlp = pipeline(\"question-generation\", model=\"valhalla/t5-small-qg-prepend\", qg_format=\"prepend\")\n",
        "spc = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "nHG0CkSegJnq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Hyperparameters for synQA Model\n",
        "\n",
        "beam = 10                   #@param {type:'number'}\n",
        "do_sampling = True          #@param [\"True\", \"False\"]\n",
        "sampling_topp = 0.9         #@param {type:'number'}\n",
        "\n",
        "decode_params = {\n",
        "    'beam': beam,\n",
        "    'sampling': do_sampling, \n",
        "    'sampling_topp': sampling_topp\n",
        "}\n",
        "\n",
        "SPECIAL_TOKENS = {\n",
        "    'bos_token': '<s>',\n",
        "    'eos_token': '</s>',\n",
        "    'sep_token': '</s>'\n",
        "}"
      ],
      "metadata": {
        "id": "0jHidWoUH_WN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Subset Input Data\n",
        "\n",
        "paragraphs = json.loads(pd.read_csv(\"paragraphs.csv\").to_json(orient=\"records\"))\n",
        "theme_wise_para = test_to_theme_wise(paragraphs)\n",
        "\n",
        "random.seed(10)\n",
        "keys_available = len(theme_wise_para.keys())\n",
        "\n",
        "num_keys = 3       #@param {type:'number'}\n",
        "num_paras = 3      #@param {type:'number'}\n",
        "\n",
        "keys = random.sample(theme_wise_para.keys(),num_keys)\n",
        "input_data = {k:theme_wise_para[k][:num_paras] for k in keys}"
      ],
      "metadata": {
        "id": "_GluRQWBGASG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Hyperparameters for KeyBert Model\n",
        "\n",
        "kw_args = {}\n",
        "kw_args['top_n'] = 10           #@param {type:'number'}\n",
        "kw_args['use_maxsum'] = True    #@param [\"True\", \"False\"]\n",
        "kw_args['diversity'] = 0.2      #@param {type:'number'}\n",
        "kw_args['nr_candidates'] = 20   #@param {type:'number'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "25CNIAtkgAVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_output = generate_qa(input_data, spc, nlp, kw_model, kw_args, generator)"
      ],
      "metadata": {
        "id": "JR8gnOmYhBTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_output.to_csv(\"/content/gdrive/MyDrive/Colab Notebooks/synthetic_data/synthetic_datat.csv\", index=False)"
      ],
      "metadata": {
        "id": "nB5RRybKrxRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}