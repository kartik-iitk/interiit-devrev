{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFUijIS4H3A6"
      },
      "outputs": [],
      "source": [
        "# Allowed to make changes.\n",
        "import collections\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import timeit\n",
        "from ast import literal_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FbdrsQAHH8Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed to make changes.\n",
        "\n",
        "# Pre-processing cell. You can use this cell to pre-process input data or load\n",
        "# your models.\n",
        "\n",
        "# Dummy code.\n",
        "paragraph_ds = []\n",
        "global_model = {}\n",
        "\n",
        "# Here, you can load the existing QA pairs for themes and pre-process it.\n",
        "# Sample QA pairs: https://drive.google.com/file/d/1HORTMN3UrPyfcibMZIA6tVnkgQXlZfWn/view?usp=share_link\n",
        "questions = []\n"
      ],
      "metadata": {
        "id": "s88yeOjL12Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed to make changes.\n",
        "def get_theme_model(theme):\n",
        "  # Here, you can load and return a model which is fine tuned for a theme.\n",
        "  # In case, you only have a global model, you can return that.\n",
        "  return global_model"
      ],
      "metadata": {
        "id": "p8RcyC4O2DFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed to make changes.\n",
        "def pred_theme_ans(questions, theme_model, pred_out):\n",
        "  theme = questions[0][\"theme\"]\n",
        "  for question in questions:\n",
        "    #-------------------------------------   \n",
        "    # add your prediction methodology here.\n",
        "    #-------------------------------------\n",
        "    # Dummy method.\n",
        "    ans = {}\n",
        "    ans[\"question_id\"] = question[\"id\"]\n",
        "    if theme == \"Kubernetes\":\n",
        "        # If no prediction for a paragraph, predict -1 for paragraph prediction\n",
        "        # and empty string for answers.\n",
        "        ans[\"paragraph_id\"] = -1\n",
        "        ans[\"answers\"] = \"\"\n",
        "    elif theme == \"ChatGPT\":\n",
        "        ans[\"paragraph_id\"] = 4\n",
        "        ans[\"answers\"] = \"2022\"\n",
        "    elif theme == \"Football world cup\":\n",
        "        ans[\"paragraph_id\"] = 6\n",
        "        ans[\"answers\"] = \"Qatar\"\n",
        "    pred_out.append(ans)"
      ],
      "metadata": {
        "id": "Aa4x0ljoIGpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes. \n",
        "\n",
        "# All theme prediction.\n",
        "questions = json.loads(pd.read_csv(\"sample_input_question.csv\").to_json(orient=\"records\"))\n",
        "theme_intervals = json.loads(pd.read_csv(\"sample_theme_interval.csv\").to_json(orient=\"records\"))\n",
        "pred_out = []\n",
        "theme_inf_time = {}\n",
        "for theme_interval in theme_intervals:\n",
        "  theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n",
        "  theme = theme_ques[0][\"theme\"]\n",
        "  # Load model fine-tuned for this theme.\n",
        "  theme_model = get_theme_model(theme)\n",
        "  execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n",
        "  theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n",
        "pred_df = pd.DataFrame.from_records(pred_out)\n",
        "pred_df.fillna(value='', inplace=True)\n",
        "# Write prediction to a CSV file. Teams are required to submit this csv file.\n",
        "pred_df.to_csv('sample_output_prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "V4MGougaIImX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes. \n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def calc_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def calc_max_f1(predicted, ground_truths):\n",
        "  max_f1 = 0\n",
        "  if len(ground_truths) == 0:\n",
        "    return len(predicted) == 0\n",
        "  for ground_truth in ground_truths:\n",
        "    f1 = calc_f1(predicted, ground_truth)\n",
        "    max_f1 = max(max_f1, f1)\n",
        "  return max_f1"
      ],
      "metadata": {
        "id": "lempoIKsIJ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # NOT allowed to make changes. \n",
        "\n",
        "# Evaluation methodology.\n",
        "metrics = {}\n",
        "pred = pd.read_csv(\"sample_output_prediction.csv\")\n",
        "pred.fillna(value='', inplace=True)\n",
        "truth = pd.read_csv(\"sample_ground_truth.csv\")\n",
        "truth.fillna(value='', inplace=True)\n",
        "truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n",
        "truth.answers = truth.answers.apply(literal_eval)\n",
        "questions = pd.read_csv(\"sample_input_question.csv\")\n",
        "for idx in pred.index:\n",
        "  q_id = pred[\"question_id\"][idx]\n",
        "  q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n",
        "  theme = q_rows[\"theme\"]\n",
        "  predicted_paragraph = pred[\"paragraph_id\"][idx]\n",
        "  predicted_ans = pred[\"answers\"][idx]\n",
        "  \n",
        "  if theme not in metrics.keys():\n",
        "    metrics[theme] = {\"true_positive\": 0, \"true_negative\": 0, \"total_predictions\": 0, \"f1_sum\": 0}\n",
        "\n",
        "  truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n",
        "  truth_paragraph_id = [ int(i) for i in truth_row[\"paragraph_id\"] ]\n",
        "  if predicted_paragraph in truth_paragraph_id:\n",
        "    # Increase TP for that theme.\n",
        "    metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n",
        "  # -1 prediction in case there is no paragraph which can answer the query.\n",
        "  if predicted_paragraph == -1 and truth_row[\"paragraph_id\"] == []:\n",
        "    # Increase TN.\n",
        "    metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n",
        "  # Increase total predictions for that theme.\n",
        "  metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n",
        "  f1 = calc_max_f1(predicted_ans, truth_row[\"answers\"])\n",
        "  metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1"
      ],
      "metadata": {
        "id": "-HA5KB3RIL4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes.\n",
        "\n",
        "# Final score.\n",
        "inf_time_threshold = 1000.0 # milliseconds.\n",
        "final_para_score = 0.0\n",
        "final_qa_score = 0.0\n",
        "# Weight would stay hidden from teams.\n",
        "theme_weights = {\"Kubernetes\": 0.5, \"ChatGPT\": 0.4, \"Football world cup\": 0.1}\n",
        "for theme in metrics:\n",
        "  inf_time_score = 1.0\n",
        "  metric = metrics[theme]\n",
        "  para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"] \n",
        "  qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n",
        "  avg_inf_time = theme_inf_time[theme] / metric[\"total_predictions\"]\n",
        "  if avg_inf_time > inf_time_threshold:\n",
        "    inf_time_score = inf_time_threshold / avg_inf_time\n",
        "  final_qa_score += theme_weights[theme] * inf_time_score * qa_score\n",
        "  final_para_score += theme_weights[theme] * inf_time_score * para_score\n",
        "print (final_para_score)\n",
        "print (final_qa_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuojd-v8INyd",
        "outputId": "d045c987-2fd7-454f-b013-c1e4196332b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7\n",
            "0.5666666666666667\n"
          ]
        }
      ]
    }
  ]
}